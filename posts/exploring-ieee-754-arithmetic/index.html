<!DOCTYPE html><html lang="en-UK" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content=""><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Exploring IEEE 754 Arithmetic" /><meta name="author" content="Mairi" /><meta property="og:locale" content="en_UK" /><meta name="description" content="Foreword" /><meta property="og:description" content="Foreword" /><link rel="canonical" href="https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/" /><meta property="og:url" content="https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/" /><meta property="og:site_name" content="Mairi’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-05-10T19:33:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Exploring IEEE 754 Arithmetic" /><meta name="twitter:site" content="@AstrumMairi" /><meta name="twitter:creator" content="@Mairi" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Mairi"},"dateModified":"2021-05-10T20:56:45+01:00","datePublished":"2021-05-10T19:33:00+01:00","description":"Foreword","headline":"Exploring IEEE 754 Arithmetic","mainEntityOfPage":{"@type":"WebPage","@id":"https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/"},"url":"https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/"}</script><title>Exploring IEEE 754 Arithmetic | Mairi's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="mask-icon" href="/assets/img/favicons/safari-pinned-tab.svg" color="#5bbad5"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> // see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> MathJax = { tex: { inlineMath: [ // start/end delimiter pairs for in-line math ['$','$'], ['\\(','\\)'] ], displayMath: [ // start/end delimiter pairs for display math ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/LOGO.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Mairi's Blog</a></div><div class="site-subtitle font-italic">Science, Research & Development</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/TMairi" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/AstrumMairi" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Exploring IEEE 754 Arithmetic</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Exploring IEEE 754 Arithmetic</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Mairi </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, May 10, 2021, 7:33 PM +0100" prep="on" > May 10 <i class="unloaded">2021-05-10T19:33:00+01:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, May 10, 2021, 7:56 PM +0000" prefix="Updated " > May 10 <i class="unloaded">2021-05-10T20:56:45+01:00</i> </span> </span></div></div><div class="post-content"><h2 id="foreword">Foreword</h2><p>This article will be covering the interesting arithmetic involved in the conversion of a hexadecimal value into an IEEE 754 (Floating-Point) value, in the context of computing. Additionally, this article will present a <code class="language-plaintext highlighter-rouge">BASH</code> script I wrote to perform such conversions, as well as provide a look into the overall process of developing and testing this script.</p><h2 id="introduction">Introduction</h2><p>Recently, in my spare time, I have been conducting a lot of extra research into the fascinating world of <a href="https://en.wikipedia.org/wiki/Reverse_engineering">reverse engineering</a>, particularly on the disassembly and analysis of binary files. While this topic is not entirely foreign to me given my line of work, I decided to learn the intricacies of a different disassembly tool alongside this research, having only been exposed to the ‘standard’ tools such as <a href="https://ghidra-sre.org/">Ghidra</a>, <a href="https://www.hex-rays.com/IDA-pro/">IDA</a> and <a href="https://www.gnu.org/software/gdb/news/reversible.html">GDB</a> thus far.</p><p>This tool I am referring to is actually a framework of disassembly tools collectively called <a href="https://github.com/radareorg/radare2">radare2</a>, or <code class="language-plaintext highlighter-rouge">r2</code>. While on the surface <code class="language-plaintext highlighter-rouge">r2</code> looks daunting, it is actually very well documented and follows a rather intuitive syntax. If you are at all unfamiliar with this tool, I highly recommend reading the official <code class="language-plaintext highlighter-rouge">radare2</code> book, which can be found <a href="https://book.rada.re/index.html">here</a>.</p><blockquote><p><strong>ATTENTION</strong>: Please note that while this article will briefly cover usage of the <code class="language-plaintext highlighter-rouge">r2</code> disassembly tool, I will not be explaining its functions in-depth, nor providing a comprehensive look into the disassembly process. In this article, <code class="language-plaintext highlighter-rouge">r2</code> is merely being used as a means to highlight a problem which formed the basis of my observation. If you would like to learn more about <code class="language-plaintext highlighter-rouge">r2</code> in-depth, please refer to the aforementioned book, or <a href="https://github.com/ifding/radare2-tutorial">this</a> tutorial to get started.</p></blockquote><h2 id="disassembling-c-variables">Disassembling C Variables</h2><p>While researching how <code class="language-plaintext highlighter-rouge">r2</code> works, I decided to test its functionality by writing some very basic <code class="language-plaintext highlighter-rouge">C</code> programs, compiling them, then disassembling the resulting binary file. This would then allow me to compare the assembly output of <code class="language-plaintext highlighter-rouge">r2</code> to the <code class="language-plaintext highlighter-rouge">C</code> source code to better understand how to navigate the disassembler, and perform analysis on more complicated binary structures.</p><p>Therefore, one of the first programs I compiled was the following <code class="language-plaintext highlighter-rouge">C</code> program, which simply lists a few common variable data-types, such as <code class="language-plaintext highlighter-rouge">int</code> (integer), <code class="language-plaintext highlighter-rouge">char</code> (character), <code class="language-plaintext highlighter-rouge">float</code> (floating-point) and <code class="language-plaintext highlighter-rouge">double</code> (higher precision floating-point):</p><div class="language-c highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">42</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">b</span> <span class="o">=</span> <span class="sc">'M'</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">5</span><span class="p">.</span><span class="mi">5</span><span class="p">;</span>
    <span class="kt">double</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">3</span><span class="p">.</span><span class="mi">14</span><span class="p">;</span>
  
<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></table></code></div></div><p>After compiling this code on my Linux system using <code class="language-plaintext highlighter-rouge">gcc</code>, I then opened the binary file using <code class="language-plaintext highlighter-rouge">r2</code> and ran the following commands (<em>output truncated</em>):</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>[0x00401020]&gt; aaa
[ . . . ]
[0x00401020]&gt; afl
[ . . . ]
[0x00401020]&gt; s main
[0x00401106]&gt; pdf
</pre></table></code></div></div><p>Here, I simply analysed the binary (<code class="language-plaintext highlighter-rouge">aaa</code>), listed the functions (<code class="language-plaintext highlighter-rouge">afl</code>), jumped to the address containing the <code class="language-plaintext highlighter-rouge">main</code> function, and then ran the ‘print disassembled function’ (<code class="language-plaintext highlighter-rouge">pdf</code>) command. The resulting output from this command was a low-level look at the <code class="language-plaintext highlighter-rouge">C</code> source code, but written in assembly (machine) language. The relevant part of this code has been provided below, wherein I also changed the <code class="language-plaintext highlighter-rouge">r2</code> variable names to make the code slightly more readable:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>push rbp
mov rbp, rsp
mov dword [int], 0x2a
mov byte [char], 0x4d
movss xmm0, dword [0x00402010]
movss dword [float], xmm0
movsd xmm0, qword [0x00402018]
movsd qword [double], xmm0
mov eax, 0
pop rbp
ret
</pre></table></code></div></div><p>Reviewing assembly code like the sample above can be quite intimidating, however, if you are somewhat familiar with programming languages in general (such as <code class="language-plaintext highlighter-rouge">C</code> and its derivatives), this code is actually quite easy to understand. Firstly the value of the base pointer register <code class="language-plaintext highlighter-rouge">rbp</code> is pushed onto the stack and then the value of the stack pointer register <code class="language-plaintext highlighter-rouge">rsp</code> is stored (copied) into it.</p><p>Continuing, the hexadecimal value <code class="language-plaintext highlighter-rouge">0x2a</code> is then stored in the <code class="language-plaintext highlighter-rouge">rbp-0x4</code> register (which I renamed to <code class="language-plaintext highlighter-rouge">int</code>, given I know the source code already) and then the value <code class="language-plaintext highlighter-rouge">0x4d</code> is stored in the <code class="language-plaintext highlighter-rouge">rbp-0x5</code> register (renamed to <code class="language-plaintext highlighter-rouge">char</code>). These two values can already be cross-referenced with our source code, as converting the them into decimal (base 10) using <code class="language-plaintext highlighter-rouge">rax2</code> gives us the value of our variables:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>[0x00401106]&gt; rax2 0x2a
42

[0x00401106]&gt; rax2 0x4d
77
</pre></table></code></div></div><p>Here we can see that our integer variable is correct at <code class="language-plaintext highlighter-rouge">42</code>, as specified in the <code class="language-plaintext highlighter-rouge">C</code> program. However, we did not specify an integer for the second variable, but a character. Therefore, if we search an ASCII table for the decimal value <code class="language-plaintext highlighter-rouge">77</code>, we will see that it corresponds to our specified character ‘M’. A handy ASCII table can be displayed within <code class="language-plaintext highlighter-rouge">r2</code> by using the following <code class="language-plaintext highlighter-rouge">rax2</code> command:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[0x00401106]&gt; rax2 -a
</pre></table></code></div></div><p>The next two variables are a little bit more complicated. You might be tempted to simply convert the hexadecimal values you see in square brackets straight to decimal, just like with the integer variable, however doing so would result in the following:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>[0x00401106]&gt; rax2 0x00402010
4202512
</pre></table></code></div></div><p>This is obviously not the correct value, and in fact, the hexadecimal string we converted does not refer to a value in of itself, but instead an address in memory. Your first clue that this may be related to floating-point numbers is the <a href="https://wiki.cheatengine.org/index.php?title=Assembler:Commands:MOVSS">movss</a> command, which moves a single-precision floating-point value from a location in memory to another operand (typically an <code class="language-plaintext highlighter-rouge">XMM</code> register). In this case, a value at the memory address <code class="language-plaintext highlighter-rouge">0x00402010</code> is being stored in the <code class="language-plaintext highlighter-rouge">xmm0</code> register, before being copied to the <code class="language-plaintext highlighter-rouge">rbp-0xc</code> (renamed to <code class="language-plaintext highlighter-rouge">float</code>) register.</p><p>But what is the value in memory being pushed to the <code class="language-plaintext highlighter-rouge">xmm0</code> register? Well we know by looking at the source code that it must be the value <code class="language-plaintext highlighter-rouge">5.5</code>, the question is how do we extrapolate this value using <code class="language-plaintext highlighter-rouge">r2</code>? In fact, there are two ways we can ascertain the value being moved into the <code class="language-plaintext highlighter-rouge">xmm0</code> register. The first involves simply looking at a hexadecimal dump of the address being moved into the <code class="language-plaintext highlighter-rouge">xmm0</code> register, as follows:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>[0x00401106]&gt; pxw @ 0x00402010
0x00402010  0x40b00000 0x00000000 0x51eb851f 0x40091eb8  ...@.......Q...@
[ . . . ]
</pre></table></code></div></div><p>In the output above, we can see the hexadecimal value <code class="language-plaintext highlighter-rouge">0x40b00000</code>, which could be our floating-point value. To check, we can simply run <code class="language-plaintext highlighter-rouge">rax2</code> again using the <code class="language-plaintext highlighter-rouge">Fx</code> parameter to specify a floating-point number:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>[0x00401106]&gt; rax2 Fx40b00000
5.500000f
</pre></table></code></div></div><p>However, we can further verify that this is the correct value by examining the contents of <code class="language-plaintext highlighter-rouge">xmm0</code> register, which brings us to the second method we can use. In this method, I create a breakpoint in the code where the value of <code class="language-plaintext highlighter-rouge">xmm0</code> is moved into the <code class="language-plaintext highlighter-rouge">float</code> variable, then I execute the binary using <code class="language-plaintext highlighter-rouge">ood</code> and <code class="language-plaintext highlighter-rouge">dc</code>. We should then hit our specified breakpoint, at which point we can query the contents of the <code class="language-plaintext highlighter-rouge">xmm0</code> register:</p><blockquote><p><strong>ATTENTION</strong>: It is not recommended to execute a binary using <code class="language-plaintext highlighter-rouge">r2</code> outside of a virtualised environment, especially when you are dealing with unknown binaries or potential malware. In this case, I created the source code for the binary being analysed, so there is no risk of damage to my system. Please exercise caution when executing binaries unless you know what you are doing.</p></blockquote><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>[0x00401115]&gt; dc
hit breakpoint at: 0x40111d

[0x0040111d]&gt; dr xmm0
0x00000000000000000000000040b00000

[0x0040111d]&gt; rax2 Fx00000000000000000000000040b00000
5.500000f
</pre></table></code></div></div><p>As you can see, we get a 16-byte hexadecimal value which matches the previous value of <code class="language-plaintext highlighter-rouge">0x40b00000</code> we found. Furthermore, as we already demonstrated earlier with <code class="language-plaintext highlighter-rouge">rax2</code>, this hexadecimal string translates into the floating-point number <code class="language-plaintext highlighter-rouge">5.5</code>. However, as I am about to show, the next variable, of the <code class="language-plaintext highlighter-rouge">double</code> data-type, is not as easy to handle.</p><p>If we repeat the process we used for the <code class="language-plaintext highlighter-rouge">float</code> variable, but this time for the <code class="language-plaintext highlighter-rouge">double</code> variable; these are the results we end up with:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre>[0x00401106]&gt; pxq @ 0x00402018
0x00402018  0x40091eb851eb851f  0x000000343b031b01   ...Q...@...;4...
[ . . . ]

[0x0040111d]&gt; dc
hit breakpoint at: 0x40112a

[0x0040112a]&gt; dr xmm0
0x000000000000000040091eb851eb851f

[0x0040112a]&gt; rax2 Fx40091eb851eb851f
126443839488.000000f
</pre></table></code></div></div><p>This is interesting, it appears that the hexadecimal value <code class="language-plaintext highlighter-rouge">0x40091eb851eb851f</code> is being moved to the <code class="language-plaintext highlighter-rouge">xmm0</code> register after the breakpoint I set. This must be our floating-point value of <code class="language-plaintext highlighter-rouge">3.14</code> as specified in the source code, however as you can see above, it does not seem like <code class="language-plaintext highlighter-rouge">rax2</code> can translate it. Looking at the manual <a href="https://r2wiki.readthedocs.io/en/latest/tools/rax2/">page</a> for the <code class="language-plaintext highlighter-rouge">rax2</code> command, it seems like the conversion between hexadecimal and floating numbers is only possible with a 32-bit hexadecimal value. However, as you can see in the output above, we have a 64-bit hexadecimal value.</p><h2 id="observation">Observation</h2><p>The answer to the conversion problem is actually quite simple; it seems that <code class="language-plaintext highlighter-rouge">rax2</code> is only capable of converting a given hexadecimal value into a single precision floating-point value. Since we set the variable <code class="language-plaintext highlighter-rouge">d</code> in our initial program as a <code class="language-plaintext highlighter-rouge">double</code> data-type, this means that the hexadecimal value needs to be converted into a double precision floating-point value.</p><p>Now the question forms; how do we reliably convert a given 64-bit hexadecimal string into a double precision floating-point value? Well, you always have the option of using online converters to do the calculations for you: <a href="https://gregstoll.com/~gregstoll/floattohex/">1</a>, <a href="https://babbage.cs.qc.cuny.edu/IEEE-754.old/32bit.html">2</a>, <a href="https://evanw.github.io/float-toy/">3</a>, <a href="https://www.h-schmidt.net/FloatConverter/IEEE754.html">4</a>. However, I am going to take a more scientific approach by examining the arithmetic outlined by the <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754</a> standard, and understanding the mathematics behind how floating-point numbers are calculated by computers.</p><p>Additionally, I will also create a <code class="language-plaintext highlighter-rouge">BASH</code> script using this arithmetic to take hexadecimal strings as input values, then convert them to floating-point values of an appropriate precision.</p><h2 id="research">Research</h2><h3 id="measurement-precision">Measurement Precision</h3><p>At this point you may be wondering what a floating-point number actually refers to, and from looking at the initialised values in the <code class="language-plaintext highlighter-rouge">C</code> source code, you will likely ascertain that they refer to numbers with decimal points. Such values are very important in the world of physics and mathematics as they allow a greater degree of precision in scientific measurements.</p><p>For instance, when you measure the distance between two objects in your household, you will likely use a standard tape measure. On this tape measure, you will be typically be able to use inches, feet, centimetres or millimetres. Using the <a href="https://en.wikipedia.org/wiki/International_System_of_Units">SI</a> units, you will not be able to measure to a greater degree of precision than the milimeter. For example, you may measure 458 mm, but you would not be able to accurately measure a distance of 458.627 mm<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p><p>In ordinary everyday-life, this level of precision will be more than sufficient for most jobs. However, it is important to understand that the level of precision you are working at depends on the <a href="https://en.wikipedia.org/wiki/Significant_figures">Significant Figures</a> of the number. For instance; the numbers we used earlier have three (458) and six (458.627) significant figures respectively. From this we can derive the <strong>most</strong> significant digit and the <strong>least</strong> significant digit, which are determined by each digits exponent value; the digit with the largest exponent is the most significant, and the digit with the smallest exponent is the least significant, as you would expect.</p><p>To further understand this concept of significant figures and exponents, which play a major factor in floating-point arithmetic, it is very important that you are familiar with <a href="https://en.wikipedia.org/wiki/Scientific_notation">Scientific Notation</a> (also known as ‘Standard Form’).</p><h3 id="scientific-notation">Scientific Notation</h3><p>In the world of physics; particularly astronomy and astrophysics, scientists will often find themselves performing calculations with exceptionally large or small numbers. Writing these numbers out normally as they appear would be inefficient and tedious. Therefore, scientific notation is commonly used instead to represent these values. For instance, the mass of a proton (in kilograms - <code class="language-plaintext highlighter-rouge">kg</code>), often denoted by \(p\) or \(p^+\), written as a real number is as follows<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>:</p><p>\(0.000000000000000000000000001672621923\)</p><p>Whereas in scientific notation, this value would be written as:</p><p>\(1.672621923 × 10^{-27}\)</p><p>You should be able to immediately see how much more convenient it is to write a very small value in this notation. We can easily illustrate this notation by converting it into a formula, as follows:</p><p>\(M × 10^n\)</p><p>In this formula, \(n\) is our <code class="language-plaintext highlighter-rouge">exponent</code> value, denoted by an integer. The \(M\) value is known as the <code class="language-plaintext highlighter-rouge">significand</code> (also commonly referred to as the <code class="language-plaintext highlighter-rouge">coefficient</code> or <code class="language-plaintext highlighter-rouge">mantissa</code>)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. Using the scientific notation for the mass of a proton above, we can say that the exponent is <code class="language-plaintext highlighter-rouge">-27</code> and the <code class="language-plaintext highlighter-rouge">significand</code> is <code class="language-plaintext highlighter-rouge">1.672621898</code>. However, it is important to note that this value can also be written as the following:</p><p>\(16.72621923 × 10^{-28}\)</p><p>\(167.2621923 × 10^{-29}\)</p><p>Hence, this is why we would say that the leading <code class="language-plaintext highlighter-rouge">1</code> is the most significant digit because its exponent is <code class="language-plaintext highlighter-rouge">-27</code>. It is also important to note for future reference that in scientific notation, the value is typically always <code class="language-plaintext highlighter-rouge">normalized</code>. This simply means that there must be one non-zero digit before the decimal point, which can be either positive or negative. In the case of the two additional notations above, they can be considered <code class="language-plaintext highlighter-rouge">de-normalized</code>, as they consist of multiple digits before the decimal point. Interestingly, this is why such values are referred to as ‘floating-point’ numbers, as the decimal point seemingly ‘floats’ between the significant digits.</p><h3 id="rounding-significant-figures">Rounding Significant Figures</h3><p>One issue commonly encountered in the realm of significant digits is that of <strong>rounding</strong>. Since we know that the number of significant digits a value has correlates to its overall precision, this means that data is lost when the value is <a href="https://en.wikipedia.org/wiki/Round-off_error">rounded off</a>. One example is the value for the speed of light in a vacuum, denoted by \(c\) and measured in metres per second (m/s), is<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>:</p><p>\(2.99792458 × 10^8\)</p><p>However, you may see this value being rounded to one significant figure, as shown below:</p><p>\(3 × 10^8\)</p><p>Although this may seem inconsequential, in actuality this may cause accuracy problems when dealing with very precise calculations. The issue of rounding when dealing with the number of significant digits can also be demonstrated with arithmetic involving decimal numbers:</p><p>\(1.8 × 4.49\)</p><p>Here we are multiplying a number with two significant digits against a number with three significant digits. A standard calculator would probably tell you that the result of this multiplication would be <code class="language-plaintext highlighter-rouge">8.08</code>, however the actual answer would be rounded to <code class="language-plaintext highlighter-rouge">8.1</code>. This is due to <code class="language-plaintext highlighter-rouge">1.8</code> only having two significant digits, meaning the answer must also have two significant digits, to account for the difference in precision between the two values. It is important to note that rounding in this way should <strong>ONLY</strong> be performed at the end of the calculation. If we were to then further multiply the original answer by <code class="language-plaintext highlighter-rouge">1.7</code> for example, you would multiply this by <code class="language-plaintext highlighter-rouge">8.08</code> and <strong>NOT</strong> <code class="language-plaintext highlighter-rouge">8.1</code>. This is done to prevent data loss in the calculation<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p><p>These types of potential precision errors all factor into floating-point arithmetic. This type of arithmetic in the computing world has been known to have problems with precision and the overall speed of calculation. To address these problems, the IEEE 754 standard was created, which prescribes how floating-point values should be represented in binary arithmetic.</p><h3 id="the-ieee-754-standard">The IEEE 754 Standard</h3><p>In 1985, the Institute of Electrical and Electronics Engineers (IEEE) published a technical standard for binary (base-2) floating-point arithmetic, which became known as <a href="https://standards.ieee.org/standard/754-1985.html">IEEE 754-1985</a>. This standard would later be superseded by <a href="https://standards.ieee.org/standard/754-2008.html">IEEE 754-2008</a>, which added a further radix format to the original, specifically radix 10 (also known as the base-10 decimal system). This standard would then again be superseded by the more recent <a href="https://standards.ieee.org/standard/754-2019.html">IEEE-754-2019</a>. If the IEEE seems familiar to you in the cyber industry, you may recognise them from their well-known <a href="https://en.wikipedia.org/wiki/IEEE_802">IEEE 802</a>, which provide a set of standards for Ethernet and wireless networks.</p><p>In its current form, IEEE 754 specifies five different floating-point formats in a ‘basic’ group. These are split into three binary formats, with lengths of 32-bits (single precision), 64-bits (double precision) and 128-bits (quadruple precision). The final two are decimal formats, with lengths of 64-bits and 128-bits<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. The three binary formats are represented in the C programming language by the <code class="language-plaintext highlighter-rouge">float</code>, <code class="language-plaintext highlighter-rouge">double</code> and <code class="language-plaintext highlighter-rouge">long double</code> data-types respectively. The standard also defines additional formats, called the ‘extended’ groups, although these are rarely used and will not be the focus of this article.</p><p>Before we dive into the storage of floating-point values, it is important to note that the IEEE 754 standard for floating-point arithmetic does more than just provide formats for these values. The standard also specifies operations such as; addition, subtraction, multiplication, etc. In addition to multiple conversions involving floating-point numbers, such as; converting integers and decimal strings to floating-point values. Finally, the standard also covers exceptions, such as dealing with Non-Numbers (NaN).</p><p>The representation of floating-point values, as defined by the IEEE 754 standard is as follows:</p><p>\((-1)^{s} × b^e × M\)</p><p>I like to think of this as a formula, which can be broken down into the following values:</p><ul><li>\(s\) : The <code class="language-plaintext highlighter-rouge">sign</code> value<li>\(b\) : The <code class="language-plaintext highlighter-rouge">base</code> system in use<li>\(e\) : The <code class="language-plaintext highlighter-rouge">exponent</code> value<li>\(M\) : The <code class="language-plaintext highlighter-rouge">significand</code> (or <code class="language-plaintext highlighter-rouge">mantissa</code>) value</ul><p>The <code class="language-plaintext highlighter-rouge">base</code> is simply our radix, which can be either <code class="language-plaintext highlighter-rouge">2</code>, to represent binary, or <code class="language-plaintext highlighter-rouge">10</code> to represent decimal. In this case, we will be working primarily with a binary radix, hence we can say at this point that: \(b = 2\) . Additionally, it is worth noting that in the IEEE 754 standard, the number of digits in the <code class="language-plaintext highlighter-rouge">significand</code> determines our level of precision, denoted by \(p\). For example; in single precision values, \(M\) would consist of 23 bits, with one additional stored as the ‘hidden bit’ (sometimes known as the ‘leading bit’), which will be explained in further detail later<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</p><p>Finally, the <code class="language-plaintext highlighter-rouge">exponent</code> is broken up into two additional parameters for granularity, these are:</p><ul><li>\(emax\) : The maximum <code class="language-plaintext highlighter-rouge">exponent</code> value<li>\(emin\) : The minimum <code class="language-plaintext highlighter-rouge">exponent</code> value</ul><p>Since we know what <code class="language-plaintext highlighter-rouge">base</code> we are working in, we simply need to extract our <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code> from the binary string we are working with to complete the conversion. In the disassembly of <code class="language-plaintext highlighter-rouge">C</code> variables earlier in the article, we found that the hexadecimal string <code class="language-plaintext highlighter-rouge">0x40b00000</code> corresponded to the floating-point value <code class="language-plaintext highlighter-rouge">5.5</code>. Using the logic outlined by the IEEE 754 standard, we can take this string as a control variable to hopefully convert it into the correct single precision floating-point value.</p><h3 id="parameter-breakdown">Parameter Breakdown</h3><p>Now we have our hexadecimal string <code class="language-plaintext highlighter-rouge">0x40b00000</code>, first of all we need to convert this into binary, since we will be working with a <code class="language-plaintext highlighter-rouge">base</code> \(b\) value of <code class="language-plaintext highlighter-rouge">2</code>. This is relatively easy to do manually, however you can use the <code class="language-plaintext highlighter-rouge">bc</code> tool on Linux to do this conversion for you. For example, the following command will convert our hexadecimal string to binary on the command-line<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span> <span class="nb">echo</span> <span class="s2">"obase=2; ibase=16; 40B00000"</span> | bc
</pre></table></code></div></div><blockquote><p><strong>ATTENTION</strong>: Please note that <code class="language-plaintext highlighter-rouge">bc</code> will strip leading 0s from the output, so be sure to restore them or use another method of conversion.</p></blockquote><p>This hexadecimal value will result in the following binary string:</p><ul><li><code class="language-plaintext highlighter-rouge">0100 0000 1011 0000 0000 0000 0000 0000</code></ul><p>We have a 32-bit binary value, which can now be broken down into our desired parameters. Since we are dealing with a single precision value; the binary string will be broken down as follows:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>BIT NO.:	|  31  | 30                   23  | 22                                                                 0 |
BINARY:		|   0  |  1  0  0  0  0  0  0  1  |  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 |
PARAMETER:	| Sign |         Exponent         |                                Significand                           |
</pre></table></code></div></div><p>Here we can see that <code class="language-plaintext highlighter-rouge">23</code> bits are allocated for the <code class="language-plaintext highlighter-rouge">significand</code>, <code class="language-plaintext highlighter-rouge">8</code> bits for the <code class="language-plaintext highlighter-rouge">exponent</code> and the final bit for our <code class="language-plaintext highlighter-rouge">sign</code>. When dealing with double (64-bit) or quadruple (128-bit) precision values, simply refer to the following table to ascertain how many bits in the binary string to assign to each parameter<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>:</p><div class="table-wrapper"><table><thead><tr><th> <th style="text-align: center">SINGLE<th style="text-align: center">DOUBLE<th style="text-align: center">QUADRUPLE<tbody><tr><td><strong>SIGN</strong><td style="text-align: center">1<td style="text-align: center">1<td style="text-align: center">1<tr><td><strong>EXPONENT</strong><td style="text-align: center">8<td style="text-align: center">11<td style="text-align: center">15<tr><td><strong>SIGNIFICAND</strong><td style="text-align: center">23<td style="text-align: center">52<td style="text-align: center">112</table></div><h3 id="sign">Sign</h3><p>First, we shall take a look at the value of the <code class="language-plaintext highlighter-rouge">sign</code>; \(s\) . In the example above, we can see that our <code class="language-plaintext highlighter-rouge">sign</code> simply has the value <code class="language-plaintext highlighter-rouge">0</code>. Therefore, substituting this value into the formula, we will get the following:</p><p>\((-1)^{0} × 2^e × M\)</p><p>Here, we can calculate \((-1)^0\) to be <code class="language-plaintext highlighter-rouge">1</code>, therefore we now know that our resulting floating-point value will be a positive number. The only other value \(s\) can possibly be is <code class="language-plaintext highlighter-rouge">1</code>, which will result in \((-1)^1\) which equals <code class="language-plaintext highlighter-rouge">-1</code>. Hence, we can reliably determine that when \(s\) equals <code class="language-plaintext highlighter-rouge">1</code>, the floating-point value is negative, and when \(s\) equals <code class="language-plaintext highlighter-rouge">0</code>, it will be positive. Finally, we can say that our formula for the conversion at this stage will now be:</p><p>\(1 × 2^e × M\)</p><h3 id="exponent">Exponent</h3><p>Now for the <code class="language-plaintext highlighter-rouge">exponent</code> value; \(e\) . In our example, the bits covering this parameter are <code class="language-plaintext highlighter-rouge">10000001</code>, which we can convert into decimal form to get our value for \(e\) . Again, this is relatively easy to do manually, however we can use the Linux tool <code class="language-plaintext highlighter-rouge">bc</code> to perform this conversion for us:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span> <span class="nb">echo</span> <span class="s2">"obase=10; ibase=2; 10000001"</span> | bc
</pre></table></code></div></div><p>This command will result in a value of <code class="language-plaintext highlighter-rouge">129</code>. Now you may be tempted to push this value straight into the formula, however there is one further variable we need to account for. The <code class="language-plaintext highlighter-rouge">exponent</code> in this case is actually <a href="https://en.wikipedia.org/wiki/Exponent_bias">biased</a>, meaning before we proceed with further calculations, the <code class="language-plaintext highlighter-rouge">biased exponent</code> must be adjusted<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</p><p>To understand what a <code class="language-plaintext highlighter-rouge">biased exponent</code> actually is, we first need to review how signed (positive or negative) integers are stored. Typically, signed integers will be represented by <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a>, which enables computers to make calculations using binary values. It follows a simple arithmetic; you have a fixed number of bits used to store the data, the most significant digit is the <code class="language-plaintext highlighter-rouge">sign</code> (as discussed earlier), the two’s complement is then calculated by inverting the bits and adding <code class="language-plaintext highlighter-rouge">1</code><sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>.</p><p>Two’s complement is very useful for storing negative numbers in binary form. For instance, if a computer wanted to store the decimal value of <code class="language-plaintext highlighter-rouge">-10</code> as two’s complement using 8-bits of data; we would first take the binary representation of decimal <code class="language-plaintext highlighter-rouge">10</code>, which would be <code class="language-plaintext highlighter-rouge">00001010</code>. Then we invert the bits (<a href="https://en.wikipedia.org/wiki/Ones%27_complement">one’s complement</a>) so the binary value becomes <code class="language-plaintext highlighter-rouge">11110101</code>. Finally, we simply add <code class="language-plaintext highlighter-rouge">1</code> to the this value, resulting in <code class="language-plaintext highlighter-rouge">11110110</code>, which is the two’s complement for the decimal value <code class="language-plaintext highlighter-rouge">-10</code>.</p><p>It is very helpful to note that in 8-bit binary values using two’s complement, the largest possible value is not <code class="language-plaintext highlighter-rouge">255</code>, which you would normally expect from <code class="language-plaintext highlighter-rouge">11111111</code>, but it is instead <code class="language-plaintext highlighter-rouge">127</code>, represented by <code class="language-plaintext highlighter-rouge">01111111</code>. This is because the first bit is the <code class="language-plaintext highlighter-rouge">sign</code>, telling us whether the value is positive or negative. Similarly, this means that the smallest possible value is <code class="language-plaintext highlighter-rouge">-127</code>, resulting from <code class="language-plaintext highlighter-rouge">11111111</code> where we take the first bit as the <code class="language-plaintext highlighter-rouge">sign</code><sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>.</p><p>Therefore, we can extrapolate that given an 8-bit <code class="language-plaintext highlighter-rouge">exponent</code>, the \(emax\) would be <code class="language-plaintext highlighter-rouge">127</code> and \(emin\) would be <code class="language-plaintext highlighter-rouge">-126</code>. If you are wondering why the \(emin\) value is not <code class="language-plaintext highlighter-rouge">-127</code>, it is simply because the IEEE standard specifies that \(emin\) shall be \(1-emax\) for all formats. Meaning <code class="language-plaintext highlighter-rouge">1-127</code> results in \(emin\) being <code class="language-plaintext highlighter-rouge">-126</code>.</p><p>Since the unsigned binary <code class="language-plaintext highlighter-rouge">exponent</code> in our floating-point calculation is not stored as two’s complement, we need to account for the <code class="language-plaintext highlighter-rouge">bias</code> (or offset). This is a very simple calculation, where the <code class="language-plaintext highlighter-rouge">bias</code> is denoted by \(k\) and uses this formula<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup>:</p><p>\(k = 2^{n-1}-1\)</p><p>Where \(n\) in this case is the number of bits in our <code class="language-plaintext highlighter-rouge">biased exponent</code>. Substituting in the value of \(n\) from our previous example; <code class="language-plaintext highlighter-rouge">8</code>, this gives us the following results:</p><p>\(k = 2^{8-1}-1\)</p><p>This results in <code class="language-plaintext highlighter-rouge">127</code>, which we already know is the \(emax\) value for 8-bit <code class="language-plaintext highlighter-rouge">exponents</code>. Therefore, to adjust for the <code class="language-plaintext highlighter-rouge">bias</code>, we simply subtract this value from our original calculation for \(e\); <code class="language-plaintext highlighter-rouge">129 - 127</code> and we find that our adjusted <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) , is <code class="language-plaintext highlighter-rouge">2</code>. With the <code class="language-plaintext highlighter-rouge">exponent</code> properly adjusted, we can now add this value to our formula, as follows:</p><p>\(1 × 2^2 × M\)</p><p>Interestingly, since we already know the number \(n\) of bits in the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) between single, double and quadruple precision values, we can pre-calculate their <code class="language-plaintext highlighter-rouge">bias</code> by using the formula for \(k\) above:</p><div class="table-wrapper"><table><thead><tr><th> <th style="text-align: center">SINGLE<th style="text-align: center">DOUBLE<th style="text-align: center">QUADRUPLE<tbody><tr><td><strong>EXPONENT BITS</strong><td style="text-align: center">8<td style="text-align: center">11<td style="text-align: center">15<tr><td><strong>BIAS FORMULA</strong><td style="text-align: center">\(2^{8-1}-1\)<td style="text-align: center">\(2^{11-1}-1\)<td style="text-align: center">\(2^{15-1}-1\)<tr><td><strong>BIAS VALUE (\(emax\))</strong><td style="text-align: center">127<td style="text-align: center">1023<td style="text-align: center">16383<tr><td><strong>BIAS VALUE (\(emin\))</strong><td style="text-align: center">-126<td style="text-align: center">-1022<td style="text-align: center">-16382</table></div><p>Now that we have interpreted the \(s\) (<code class="language-plaintext highlighter-rouge">sign</code>) and the \(e\) (<code class="language-plaintext highlighter-rouge">exponent</code>) values, it is time to move onto the final piece of the puzzle; the \(M\) (<code class="language-plaintext highlighter-rouge">significand</code>).</p><h3 id="significand-mantissa">Significand (Mantissa)</h3><p>Put simply; the <code class="language-plaintext highlighter-rouge">significand</code> is the part of the floating-point value which contains its significant digits. You may recall that the number of significant digits in the <code class="language-plaintext highlighter-rouge">significand</code> correlates to the level of precision we are working with. In our example floating-point conversion, we are dealing with a 32-bit single precision value, which gives us a 23-bit <code class="language-plaintext highlighter-rouge">signficand</code>. The binary value for \(M\) in our calculation was found to be:</p><ul><li><code class="language-plaintext highlighter-rouge">01100000000000000000000</code></ul><p>The next step is to convert this value into a floating-point <code class="language-plaintext highlighter-rouge">significand</code> to then be substituted into our formula. To do this, we need to account for the ‘hidden bit’ we mentioned earlier. When dealing with normal numbers, there is a leading <code class="language-plaintext highlighter-rouge">1</code> which is added to the <code class="language-plaintext highlighter-rouge">significand</code>, which is <em>implied</em> under normal circumstances. The reason this <code class="language-plaintext highlighter-rouge">1</code> is added is due to the way the <code class="language-plaintext highlighter-rouge">normalization</code> process works, as we mentioned before when discussing scientific notation; the most significant digit <strong>cannot</strong> be zero. Thus, since we are working with a binary value, if the value cannot be <code class="language-plaintext highlighter-rouge">0</code>, it must instead be <code class="language-plaintext highlighter-rouge">1</code><sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">14</a></sup>.</p><p>Therefore, if we add our implied bit to our example <code class="language-plaintext highlighter-rouge">significand</code>, we get the following value:</p><ul><li><code class="language-plaintext highlighter-rouge">1.01100000000000000000000</code></ul><p>It is worth noting at this stage, that we can omit the trailing zeros from the <code class="language-plaintext highlighter-rouge">significand</code>, leaving us with <code class="language-plaintext highlighter-rouge">1.011</code>. Substituting this value into our formula (and re-arranging it a bit), we will form the following:</p><p>\(1.011 × 2^2\)</p><p>Note how we have also omitted the <code class="language-plaintext highlighter-rouge">sign</code> value as it simply determines whether our resulting value is positive or negative, and we already know this value will be positive. The above may look familiar to a value in scientific notation, meaning we can extrapolate the full value by expanding the formula to give us the following:</p><ul><li><code class="language-plaintext highlighter-rouge">101.1</code></ul><p>Now it is a simple case of converting this value to its final decimal equivalent. Again, the mathematics behind this is very simple; we simply perform \(2^x\) where \(x\) is a set bit value. The following illustrates this in action:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>BIT VALUE: | 2 | 1 | 0 | . | -1 |
BINARY:    | 1 | 0 | 1 | . |  1 |
</pre></table></code></div></div><p>As you can see, only the bit values <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">-1</code> are set. Hence the final calculation we perform is the following:</p><p>\(2^2 + 2^0 + 2^{-1} = 4 + 1 + 0.5\)</p><p>This results in our floating-point value of <code class="language-plaintext highlighter-rouge">5.5</code>, which is correct to what we set in the original <code class="language-plaintext highlighter-rouge">C</code> program. However, this is not the end of the discussion about the <code class="language-plaintext highlighter-rouge">significand</code>, as we have certain exceptions to deal with, such as ‘subnormal’ values.</p><h3 id="subnormal-values">Subnormal Values</h3><p>As we laid out in the <code class="language-plaintext highlighter-rouge">significand</code>, the ‘hidden bit’ is added to the mantissa when we are dealing with normal numbers. In this case, a normal number is defined as any value where the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) is greater than <code class="language-plaintext highlighter-rouge">-126</code>, but less than <code class="language-plaintext highlighter-rouge">128</code>:</p><p>\(-126&lt;e&lt;128\)</p><p>However, what happens when our <code class="language-plaintext highlighter-rouge">biased exponent</code> is <code class="language-plaintext highlighter-rouge">0</code>? Such an event occurs when all the bits in the <code class="language-plaintext highlighter-rouge">exponent</code> are <code class="language-plaintext highlighter-rouge">0</code>, meaning when we adjust for the <code class="language-plaintext highlighter-rouge">bias</code>, the resulting value for \(e\) will be <code class="language-plaintext highlighter-rouge">-127</code>, which falls outside of our defined \(emin\) number for single precision values<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">15</a></sup>. According to the IEEE 754 standard, when \(e=0\) and \(M \ne 0\) , then the value is ‘subnormal’ and needs a slightly different calculation. In such instances, the formula we would use is as follows:</p><p>\((-1)^{s} × 2^{emin} × M\)</p><p>Essentially, the ‘hidden bit’ becomes <code class="language-plaintext highlighter-rouge">0</code> when dealing with subnormal values. For instance, the hexadecimal value <code class="language-plaintext highlighter-rouge">0x007fffff</code>, when converted into a single precision floating-point value, represents the largest subnormal number possible for this precision level. Breaking this hexadecimal string down into binary format to extract the <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code> illustrates this:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>BIT NO.:	|  31  | 30                   23  | 22                                                                 0 |
BINARY:		|   0  |  0  0  0  0  0  0  0  0  |  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 |
PARAMETER:	| Sign |         Exponent         |                                Significand                           |
</pre></table></code></div></div><p>As you can see; the <code class="language-plaintext highlighter-rouge">sign</code> is <code class="language-plaintext highlighter-rouge">0</code> so we know the final value will be positive. The <code class="language-plaintext highlighter-rouge">biased exponent</code> is <code class="language-plaintext highlighter-rouge">0</code>, so adjusting this, we get <code class="language-plaintext highlighter-rouge">-127</code> for our <code class="language-plaintext highlighter-rouge">exponent</code>, telling us that the number is subnormal. The <code class="language-plaintext highlighter-rouge">significand</code> is then prepended with a <code class="language-plaintext highlighter-rouge">0</code> since the value is subnormal, which then gives us the following:</p><ul><li><code class="language-plaintext highlighter-rouge">0.11111111111111111111111</code></ul><p>Converting this value into decimal is actually very simple given the fact that every bit is <code class="language-plaintext highlighter-rouge">1</code>. Since we know there are twenty-three bits in the fractional part of the <code class="language-plaintext highlighter-rouge">significand</code>, we can simply perform the following calculation to figure out the value of \(M\):</p><p>\(1 - 2^{-23}\)</p><p>In this case, this is the same as performing \(2^{-1} + 2^{-2}+ … +2^{-23}\) and results in the value <code class="language-plaintext highlighter-rouge">0.99999988079071</code>. Now we simply substitute in our values into the equation for subnormal numbers:</p><p>\((-1)^{0} × 2^{-126} × 0.99999988079071\)</p><p>Rearranging this slightly and we end up with:</p><p>\(0.99999988079071 × 2^{-126}\)</p><p>Note that in this equation, our <code class="language-plaintext highlighter-rouge">exponent</code> value is \(emin\) . This results in the largest possible subnormal value of \(1.17549421069244 × 10^{-38}\) , which is still a very small number regardless. For reference; the hexadecimal string <code class="language-plaintext highlighter-rouge">0x00000001</code> will result in the smallest possible subnormal single precision floating-point value.</p><p>Now we have learned how to appropriately deal with subnormal values, there are a few other exceptions that you will need to be aware of when converting hexadecimal strings into floating-point numbers.</p><h3 id="signed-zero">Signed Zero</h3><p>While testing out IEEE 754 floating-point conversions, you may attempt to calculate the floating-point value of a hexadecimal string consisting of all zeroes, such as <code class="language-plaintext highlighter-rouge">0x00000000</code>. This is interesting, because you may be able to look at this and immediately realise that the resulting floating-point value must also be <code class="language-plaintext highlighter-rouge">0</code>. However, how can you ascertain whether it is positive or negative?</p><p>Ordinarily, the number <code class="language-plaintext highlighter-rouge">0</code> does not have a sign, but when dealing with IEEE 754 floating point values, this is not the case, and we can obtain the result of a <a href="https://en.wikipedia.org/wiki/Signed_zero">signed zero</a>. In terms of the floating-point arithmetic we have laid out thus far, a signed zero will occur when the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) and <code class="language-plaintext highlighter-rouge">significand</code> \(M\) values are all zeroes. The only difference between a positive and negative zero is dependant on the value of the <code class="language-plaintext highlighter-rouge">sign</code> \(s\).</p><p>Therefore, we can say that there are two further special exceptions in the floating-point arithmetic which occur when \(e\) and \(M\) are all zeroes. A positive zero can be obtained by converting the hexadecimal string <code class="language-plaintext highlighter-rouge">0x00000000</code> into a single precision floating-point value, and a negative zero can be obtained from the hexadecimal string <code class="language-plaintext highlighter-rouge">0x80000000</code>. The reason this particular string results in a negative zero is very easy to notice when written in binary format:</p><ul><li><code class="language-plaintext highlighter-rouge">1000 0000 0000 0000 0000 0000 0000 0000</code></ul><p>As you can see, the <code class="language-plaintext highlighter-rouge">sign</code> \(s\) value is <code class="language-plaintext highlighter-rouge">1</code>, which immediately informs us that the resulting value will be negative, as per \((-1)^1\) . Since the rest of the binary value are zeroes, we can simply say that the resulting floating-point number is <code class="language-plaintext highlighter-rouge">-0</code><sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup>.</p><h3 id="signed-infinity">Signed Infinity</h3><p>Another special quantity which can arise from IEEE 754 floating-point calculations is either positive or negative infinity. Looking through the logic we have demonstrated so far, you may have noticed that we ended up with a subnormal value when the adjusted <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) was <code class="language-plaintext highlighter-rouge">-126</code>. However, what happens when the value of \(e\) is on the other end of the spectrum at <code class="language-plaintext highlighter-rouge">128</code>?</p><p>This event occurs when all of the bits in the <code class="language-plaintext highlighter-rouge">exponent</code> are <code class="language-plaintext highlighter-rouge">1</code>, meaning when we calculate the <code class="language-plaintext highlighter-rouge">biased exponent</code>, it will be <code class="language-plaintext highlighter-rouge">255</code> (for single precision numbers). Adjusting this value to account for the <code class="language-plaintext highlighter-rouge">bias</code>, we end up with <code class="language-plaintext highlighter-rouge">128</code> as the value for \(e\) , which falls outside of our defined range for normal and subnormal numbers<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">17</a></sup>.</p><p>It should be noted in this case that the term ‘inifinity’ is used to refer to floating-point values which would otherwise cause an overflow. In the context of binary floating-point computation; an ‘overflow’ pertains to the problems computers will encounter when representing very large numbers. Similarly, the term ‘underflow’ is used to describe the issue of computers representing very small numbers.</p><p>Therefore, the IEEE 754 standard accounts for this and specifies that when the value of \(e\) is \(emax + 1\) and the <code class="language-plaintext highlighter-rouge">significand</code> \(M\) is all <code class="language-plaintext highlighter-rouge">0</code>, then the result will be INF (infinity). As seen before with signed zero, the infinity result can also be either positive or negative, depending on the value of the <code class="language-plaintext highlighter-rouge">sign</code> \(s\) bit.</p><p>According to these rules, in terms of single precision floating-point values; the hexadecimal string <code class="language-plaintext highlighter-rouge">0x7f80000</code> will result in positive infinity (+INF), while the hexadecimal string <code class="language-plaintext highlighter-rouge">0xff800000</code> will result in negative infinity (-INF).</p><h3 id="nan-non-numbers">NaN (Non-Numbers)</h3><p>The final special quantity which can arise from IEEE 754 floating-point calculations is <a href="https://en.wikipedia.org/wiki/NaN">NaN</a>, or Not-a-Number (commonly referred to as Non-Numbers). NaNs occur in floating-point values when the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) is \(emax +1\) , similar to the aforementioned signed infinity values. However, this time the difference being that the <code class="language-plaintext highlighter-rouge">significand</code> \(M\) is non-zero<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">18</a></sup>.</p><p>A non-number simply refers to an invalid computational mathematical operation, the most famous example being the division of any number by <code class="language-plaintext highlighter-rouge">0</code>. Another example is attempting to perform the square root of a negative number. In the case of floating-point values, the non-zero <code class="language-plaintext highlighter-rouge">significand</code> separates NaNs from signed infinity and becomes an impossible mathematical operation because you cannot use infinity in arithmetic.</p><p>Thus, the IEEE 754 standard specifies that in cases where the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) value is \(emax + 1\) and the value of the <code class="language-plaintext highlighter-rouge">significant</code> \(M\) is non-zero; the resulting floating-point number will be considered NaN.</p><p>Interestingly, it is worth noting that unlike the signed zero and signed infinity exceptions, the <code class="language-plaintext highlighter-rouge">sign</code> \(s\) bit in the computation of Non-Numbers makes no difference. In other words; there is no distinction between positive NaN or negative NaN. Therefore, we can say, for example; the hexadecimal string <code class="language-plaintext highlighter-rouge">0xffc00001</code> will result in a NaN. This is much easier to ascertain by examining this string in binary form:</p><ul><li><code class="language-plaintext highlighter-rouge">1 11111111 10000000000000000000001</code></ul><h3 id="addressing-the-observation">Addressing the Observation</h3><p>My original observation formed the basis for the aforementioned research into how binary floating-point numbers are interpreted by computers using the IEEE 754 standard. In my case, I wanted to understand how the hexadecimal value <code class="language-plaintext highlighter-rouge">0x40091eb851eb851f</code>, stored in the <code class="language-plaintext highlighter-rouge">xmm0</code> register, converts into the floating-point number <code class="language-plaintext highlighter-rouge">3.14</code>, as written in the <code class="language-plaintext highlighter-rouge">C</code> source code.</p><p>Although much of my research focused on single precision values, the logic is very easy to adjust for double precision values. In this case, since the hexadecimal string <code class="language-plaintext highlighter-rouge">0x40091eb851eb851f</code> is 64-bits long, it will result in a double precision floating point number. Firstly, I will convert this string into binary to then extract the <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code>:</p><ul><li><code class="language-plaintext highlighter-rouge">0100000000001001000111101011100001010001111010111000010100011111</code></ul><p>In the case of double precision values, the <code class="language-plaintext highlighter-rouge">exponent</code> \(e\) consists of <code class="language-plaintext highlighter-rouge">11</code> bits (with a <code class="language-plaintext highlighter-rouge">bias</code> of <code class="language-plaintext highlighter-rouge">1023</code>) and the <code class="language-plaintext highlighter-rouge">significand</code> \(M\) consists of <code class="language-plaintext highlighter-rouge">52</code> bits, as follows:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>SIGN:               0 (Number is positive)
BIASED EXPONENT:    10000000000 (Decimal: 1024)
ADJUSTED EXPONENT:  1024 - 1023 = 1
MANTISSA:           1001000111101011100001010001111010111000010100011111
</pre></table></code></div></div><p>By breaking down the binary into these parts, we can quickly ascertain whether we are dealing with a subnormal or other special quantity. In this case, we have a normal number, so no exceptional calculations need to factor into our conversion. If you will recall, the formula for the conversion that we will use for normal numbers is as follows:</p><p>\((-1)^{s} × 2^{e-bias} × M\)</p><p>We already know \(s\) is <code class="language-plaintext highlighter-rouge">0</code> so the result will be positive. The adjusted <code class="language-plaintext highlighter-rouge">biased exponent</code> \(e\) is <code class="language-plaintext highlighter-rouge">1</code> and the <code class="language-plaintext highlighter-rouge">significand</code> \(M\) will be <code class="language-plaintext highlighter-rouge">1.1001000111101011100001010001111010111000010100011111</code>. To adjust the <code class="language-plaintext highlighter-rouge">singificand</code>, we use the arithmetic we outlined earlier, which will give us a final \(M\) value of:</p><ul><li><code class="language-plaintext highlighter-rouge">1.57000000000000006217</code></ul><p>Now we can substitute these values into the formula, providing us with our final calculation in scientific notation, as follows:</p><p>\(1.57000000000000006217 × 2^{1}\)</p><p>Which results in our double precision floating-point value of <code class="language-plaintext highlighter-rouge">3.14</code>. Therefore, I have been able to successfully apply the arithmetic outlined in the IEEE 754 standard to convert a 64-bit hexadecimal value into a floating-point number, which I was previously unable to do within the disassembly tool.</p><p>The next step I decided to take was to create a shell script, written entirely using <code class="language-plaintext highlighter-rouge">BASH</code> logic, and no reliance on external tools, with the goal of converting hexadecimal strings to floating-point values straight from the Linux command-line. After carefully reviewing my research up to this point, I was confident I could write a functioning <code class="language-plaintext highlighter-rouge">BASH</code> script, so I formulated a hypothesis.</p><h2 id="hypothesis">Hypothesis</h2><p>I will admit that <code class="language-plaintext highlighter-rouge">BASH</code> is a very limiting ‘language’ to script in, and although I would prefer to use something like <code class="language-plaintext highlighter-rouge">C</code>, I could see that such conversion programs had already been written, whereas I could not find any examples of <code class="language-plaintext highlighter-rouge">BASH</code> scripts which handled floating-point conversions. Normally, I would be under the assumption that if someone has not done it before me, then it is likely impossible, or rather, impractical to do so.</p><p>Nevertheless, my research supports the idea that such conversions are possible using <code class="language-plaintext highlighter-rouge">BASH</code>, therefore, I posed my hypothesis; <em>It is possible to reliably convert hexadecimal strings to precision-appropriate floating-point values using BASH script</em>. The next step, as with all (good) hypotheses, was to perform testing and experimentation to either prove or disprove my hypothesis.</p><h2 id="experimentation--testing">Experimentation / Testing</h2><h3 id="environment">Environment</h3><p>Development and testing of this shell script was performed solely on a Linux machine running the <a href="https://getfedora.org/">Fedora</a> 32 distribution. It is also important to note that, due to having to factor rounding into certain <code class="language-plaintext highlighter-rouge">BASH</code> calculations, I was running <code class="language-plaintext highlighter-rouge">BASH</code> version 5.0.17. Finally, this machine uses an Intel-based processor, which is also significant as different processors, such as ARM, may use different levels of precision in floating-point calculations.</p><h3 id="conversions">Conversions</h3><p>When I write a shell script, or any program, I typically write small portions of the code, verify that code is working as intended, and then expand upon it as needed. In this case, I knew that I would need to convert the hexadecimal input string to binary to perform the floating-point calculations, so I decided to add simple binary conversion functionality to the script at the same time.</p><p>I wanted to avoid using external Linux tools like <code class="language-plaintext highlighter-rouge">bc</code> to do the arithmetic, so I decided to simply map each possible hexadecimal character to its corresponding binary value in a <code class="language-plaintext highlighter-rouge">case</code> statement, which worked perfectly. This method would also eliminate the issue of leading zeroes being removed from the output, which is a problem with tools like <code class="language-plaintext highlighter-rouge">bc</code>.</p><p>Just because I could, and to make the script a bit more useful, I also added conversions from hexadecimal to ASCII and decimal formats. Again, this was very straightforward to do with <code class="language-plaintext highlighter-rouge">BASH</code> logic. For conversion into ASCII, I simply used a one-line <code class="language-plaintext highlighter-rouge">for</code> loop, as follows:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="k">for </span>char <span class="k">in</span> <span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="nv">$hex_string</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s2">"s/</span><span class="se">\(</span><span class="s2">..</span><span class="se">\)</span><span class="s2">/</span><span class="se">\1</span><span class="s2"> /g"</span> | <span class="nb">tr</span> <span class="nt">-d</span> <span class="s1">'\x00'</span><span class="si">)</span><span class="p">;</span> <span class="k">do </span><span class="nb">printf</span> <span class="sb">`</span><span class="nb">echo</span> <span class="s2">"</span><span class="se">\x</span><span class="nv">$char</span><span class="s2">"</span><span class="sb">`</span><span class="p">;</span><span class="k">done</span>
</pre></table></code></div></div><p>This loop iterates through each hexadecimal character, and uses <code class="language-plaintext highlighter-rouge">printf</code> to display the character in ASCII format. It should be noted that during testing I had issues with NULL bytes, so I ended up having to use <code class="language-plaintext highlighter-rouge">tr</code> to trim them from the input. It was at this point I also implemented rigorous user input sanitisation for all functions, to prevent incorrect strings from being treated as hexadecimal.</p><p>Conversion from hexadecimal (base-16) into decimal (base-10) was even easier using the following one-liner:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nb">echo</span> <span class="nt">-n</span> <span class="s2">"</span><span class="nv">$hex_string</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s1">'s|0x||g'</span> | <span class="nb">sed</span> <span class="s1">'s| ||g'</span> | <span class="nb">sed</span> <span class="s1">'s|^|0x|'</span> | xargs <span class="nb">printf</span> <span class="s2">"%f</span><span class="se">\n</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s1">'s|\..*||g'</span>
</pre></table></code></div></div><p>As with the ASCII conversion, this simply uses <code class="language-plaintext highlighter-rouge">printf</code> in conjunction with <code class="language-plaintext highlighter-rouge">xargs</code> to display the hexadecimal input string in decimal format.</p><h3 id="floating-point-functions">Floating-Point Functions</h3><p>Writing the functions covering the extraction of the <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">biased exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code> from the hexadecimal input string were a bit more challenging. Firstly, I would convert the input into binary and then use an <code class="language-plaintext highlighter-rouge">if</code> statement to check how long the input string was. I decided to write functions covering conversions from an input value of 16-bits (Half precision), 32-bits (single precision), 64-bits (double precision) and 128-bits (quadruple precision).</p><p>Starting with single precision values, I used <code class="language-plaintext highlighter-rouge">awk</code> to separate the binary string into the <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code>. I then wrote a variable to adjust the decimal <code class="language-plaintext highlighter-rouge">exponent</code> value based on the <code class="language-plaintext highlighter-rouge">bias</code> for the precision level I was working at (e.g. 127 for single precision). Dealing with the <code class="language-plaintext highlighter-rouge">significand</code> took a little more tweaking, since there is no method (that I know of) in <code class="language-plaintext highlighter-rouge">BASH</code> to convert a binary string with a decimal point into a base-10 number.</p><p>Therefore, I came up with the idea to initialise the mantissa as an array and then iterate through each bit, mapped to another numbered array, using a <code class="language-plaintext highlighter-rouge">for</code> loop and an <code class="language-plaintext highlighter-rouge">awk</code> calculation to perform \(2^x\) where \(x\) is every bit equal to <code class="language-plaintext highlighter-rouge">1</code>. This sounds more complicated than it actually is, but then all I had to do from here was add the resulting values together and I get my decimal value for \(M\) .</p><p>Substituting these values into the formula for floating-point calculations was simple using a command in <code class="language-plaintext highlighter-rouge">gawk</code>. For instance, the <code class="language-plaintext highlighter-rouge">gawk</code> command I used for the final calculation for double precision normal floating-point numbers was as follows:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>gawk <span class="nt">-M</span> <span class="nt">-v</span> <span class="nv">ROUNDMODE</span><span class="o">=</span><span class="s2">"Z"</span> <span class="s1">'{print (-1)**'</span><span class="nv">$sign</span><span class="s1">'*2**('</span><span class="nv">$biased_exponent</span><span class="s1">'-1023)*(1+'</span><span class="nv">$significand</span><span class="s1">')}'</span>
</pre></table></code></div></div><p>Here, you can see that I adjust the <code class="language-plaintext highlighter-rouge">biased exponent</code> by subtracting <code class="language-plaintext highlighter-rouge">1023</code> from the value, as per the double precision value in this case. Since this is a normal number, <code class="language-plaintext highlighter-rouge">1</code> is added to the <code class="language-plaintext highlighter-rouge">significand</code> to account for the ‘hidden bit’. You will note that I had to use <code class="language-plaintext highlighter-rouge">gawk</code> so I could take advantage of the <code class="language-plaintext highlighter-rouge">-M</code> parameter, which performs all integer arithmetic using <a href="https://gmplib.org/">GMP</a> arbitrary-precision integers. The <code class="language-plaintext highlighter-rouge">ROUNDMODE</code> variable passed to <code class="language-plaintext highlighter-rouge">gawk</code> changes the rounding mode to use for arbitrary precision arithmetic on numbers.</p><p>When it came to dealing with subnormal numbers, I used an <code class="language-plaintext highlighter-rouge">if</code> statement to check the \(e\) and \(M\) values to ensure they matched the criteria for subnormals. Then I could substitute the values into the following calculation (double precision in this case):</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>gawk <span class="nt">-M</span> <span class="nt">-v</span> <span class="nv">ROUNDMODE</span><span class="o">=</span><span class="s2">"Z"</span> <span class="s1">'{print (-1)**'</span><span class="nv">$sign</span><span class="s1">'*2**(-1022)*'</span><span class="nv">$significand</span><span class="s1">'}'</span>
</pre></table></code></div></div><p>The difference here is that there is no need to specify the <code class="language-plaintext highlighter-rouge">exponent</code> as a variable. For subnormals, the value of \(e\) will be equal to \(emin\), which in this case would always be <code class="language-plaintext highlighter-rouge">-1022</code> for double precision floating-point values. Finally, the only other difference is that the <code class="language-plaintext highlighter-rouge">significand</code> is left as is, with the ‘hidden bit’ at <code class="language-plaintext highlighter-rouge">0</code>.</p><p>The last issues I had to account for were the exceptions for the special quantities; signed zero, signed infinity and NaN. Again, this was very easy to implement as multiple <code class="language-plaintext highlighter-rouge">if</code> statements to check (and compare) the values of the <code class="language-plaintext highlighter-rouge">sign</code>, <code class="language-plaintext highlighter-rouge">exponent</code> and <code class="language-plaintext highlighter-rouge">significand</code> to understand how to treat the output value. For instance, should the value of \(e\) be <code class="language-plaintext highlighter-rouge">0</code>, \(M\) be non-zero and \(s\) be <code class="language-plaintext highlighter-rouge">1</code> then the result would be negative zero.</p><p>Unfortunately, one of the issues I ran into when testing this script early on was with the rounding of very small numbers. For instance, when I supplied a hexadecimal input string such as <code class="language-plaintext highlighter-rouge">0x00800000</code>, which would normally convert into the smallest possible normal number of \(1.1754943508 × 10^{−38}\) , it would instead output <code class="language-plaintext highlighter-rouge">0.0000000000000000000000000000</code>. I eventually realised that this was an issue with the rounding which I could not accurately account for. Hence why I had to use <code class="language-plaintext highlighter-rouge">gawk</code> as outlined earlier.</p><h2 id="data-analysis">Data Analysis</h2><p>Once the bulk of the <code class="language-plaintext highlighter-rouge">BASH</code> script functionality was written and tested, I could start analysing the results. For my control variables, I took the hexadecimal input values which covered each of the following:</p><ul><li>Normal Numbers<li>Subnormal Numbers<li>Signed Zero<li>Signed Infinity<li>Non-Numbers (NaN)</ul><p>I then fed these input values into each of the four functions I had created for IEEE 754 binary floating-point conversions; half, single, double and quadruple precision. Each one I tested yielded the correct floating-point value with no obvious errors in my experimentation. With this said, I decided that the final test would be my original floating-point <code class="language-plaintext highlighter-rouge">C</code> program variables I specified at the beginning of this article. As a reminder; in this program I used a single precision value of <code class="language-plaintext highlighter-rouge">5.5</code>, and a double precision value of <code class="language-plaintext highlighter-rouge">3.14</code>.</p><p>When I disassembled this program using <code class="language-plaintext highlighter-rouge">r2</code>, I found that the hexadecimal value of the single precision floating-point number was <code class="language-plaintext highlighter-rouge">0x40b00000</code>. Using my script, which I named <code class="language-plaintext highlighter-rouge">HexConv</code> (very original I know), with the <code class="language-plaintext highlighter-rouge">-fs</code> parameter, resulted in the following:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nv">$ </span> ./hexconv <span class="nt">-fs</span> 40b00000
SIGN <span class="o">(</span>S<span class="o">)</span>:				0
EXPONENT <span class="o">(</span>E<span class="o">)</span>:				10000001 <span class="o">(</span>Dec. 129<span class="o">)</span>
MANTISSA <span class="o">(</span>M<span class="o">)</span>:				01100000000000000000000
EXPONENT BIAS ADJUST:			2

SINGLE-PRECISION FLOATING NUMBER:	5.5000000000000000000000000000
</pre></table></code></div></div><p>As you can see, the converted floating-point value was <code class="language-plaintext highlighter-rouge">5.5</code>, with a few trailing zeros, which is a symptom of the rounding problem I had to circumvent. Lastly, I also tested the double precision value, whose hexadecimal string was found in <code class="language-plaintext highlighter-rouge">r2</code> to be <code class="language-plaintext highlighter-rouge">0x40091eb851eb851f</code>. Again, my <code class="language-plaintext highlighter-rouge">HexConv</code> script yielded the following results:</p><div class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nv">$ </span> ./hexconv <span class="nt">-fd</span> 40091eb851eb851f
SIGN <span class="o">(</span>S<span class="o">)</span>:				0
EXPONENT <span class="o">(</span>E<span class="o">)</span>:				10000000000 <span class="o">(</span>Dec. 1024<span class="o">)</span>
MANTISSA <span class="o">(</span>M<span class="o">)</span>:				1001000111101011100001010001111010111000010100011111
EXPONENT BIAS ADJUST:			1

DOUBLE-PRECISION FLOATING NUMBER:	3.1399999879323230445038461766671389341354370117187500
</pre></table></code></div></div><p>You may be wondering why the resulting floating-point value is not exactly <code class="language-plaintext highlighter-rouge">3.14</code>, well this is again due to the fact I specify a much higher level of precision in my script calculations to account for the rounding problems. However, this value is still functionally <code class="language-plaintext highlighter-rouge">3.14</code>.</p><h2 id="concluding-statements">Concluding Statements</h2><p>Satisfied with the results that my <code class="language-plaintext highlighter-rouge">HexConv</code> script were producing, I published the script on my <a href="https://github.com/TMairi">GitHub</a> page, and the script itself can be found <a href="https://github.com/TMairi/HexConv/blob/master/hexconv">here</a>. As I stated previously, I am aware that converting hexadecimal strings to IEEE 754 floating-point values using <code class="language-plaintext highlighter-rouge">BASH</code> may not be the most efficient nor the most practical method of doing so. However, I figured that scripting the arithmetic outlined in my research using <code class="language-plaintext highlighter-rouge">BASH</code> would give me a much greater understanding of the fascinating mathematics involved in these conversions. Additionally, since I could not find any working examples of shell scripts which performed IEEE 754 conversions, I figured it would also be a decent test of my <code class="language-plaintext highlighter-rouge">BASH</code> knowledge, since I had nothing to compare it to.</p><p>Interestingly, this topic of research was sparked by my interest in learning more about the <code class="language-plaintext highlighter-rouge">r2</code> disassembly framework. On this topic, I will say that I am very much enjoying using <code class="language-plaintext highlighter-rouge">r2</code> as I find its syntax very easy to understand and is a fantastic tool for someone like me, who likes to work primarily on the command-line. Although this article was not an in-depth look into <code class="language-plaintext highlighter-rouge">r2</code> or reverse engineering in general, once I have experimented more with disassembly using <code class="language-plaintext highlighter-rouge">r2</code>, I will likely publish any interesting findings here, which will be more technical than mathematical.</p><p>Finally, I would like to conclude this article by stating that; understanding the arithmetic behind how computers interpret strings as floating-point values can be exceptionally useful for people working in the cyber industry. I know from experience that a lot of my junior colleagues would be resistant to learning assembly or computational mathematics. I understand that these can be daunting or even boring subjects, but I always find that those who frequently ask <em>“why?”</em>, will invariably become better analysts.</p><p>Indeed, as with everything in science and especially in cyber, you should always strive to understand <em>why</em> something works. In the context of this article, that ‘something’ for me was the IEEE 754 arithmetic, and as mathematics is the language of science, I was captivated to take the time to learn how it worked.</p><p>If you have any recommendations or questions about the topics mentioned in this article, you can contact me on <a href="https://twitter.com/AstrumMairi">Twitter</a>.</p><p>– Mairi</p><h2 id="references">References</h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>LibreTexts. (2021). <a href="https://chem.libretexts.org/@go/page/47448">Significant Figures - Writing Numbers to Reflect Precision</a> [Accessed: 2021-05-08] <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>CODATA. (2018). <a href="https://physics.nist.gov/cgi-bin/cuu/Value?mp">CODATA Value: proton mass</a> [Accessed: 2021-05-08] <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>Harries, I. (2004). <a href="https://www.doc.ic.ac.uk/~eedwards/compsys/float/">Floating Point Numbers</a> [Accessed: 2021-05-08] <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>CODATA. (2018). <a href="https://physics.nist.gov/cgi-bin/cuu/Value?c">CODATA Value: speed of light in vacuum</a> [Accessed: 2021-05-08] <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>Lower, S. (2021). <a href="https://chem.libretexts.org/Bookshelves/General_Chemistry/Book%3A_Chem1_%28Lower%29/04%3A_The_Basics_of_Chemistry/4.06%3A_Significant_Figures_and_Rounding">Significant Figures and Rounding</a> [Accessed: 2021-05-08] <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>IEEE. (2019). <a href="https://standards.ieee.org/standard/754-2019.html">IEEE 754-2019 - IEEE Standard for Floating-Point Arithmetic</a> [Accessed: 2021-05-08] <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:7" role="doc-endnote"><p>Goldberg, D. (1991). <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a> [Accessed: 2021-05-08] <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:8" role="doc-endnote"><p>StackOverflow. (2012). <a href="https://stackoverflow.com/questions/11120324/hex-to-binary-conversion-in-bash">Hex to Binary conversion in bash</a> [Accessed: 2021-05-08] <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:9" role="doc-endnote"><p>Finley, T. (2000). <a href="https://www.cs.cornell.edu/~tomf/notes/cps104/floating.html">Floating Point</a> [Accessed: 2021-05-08] <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:10" role="doc-endnote"><p>Irvine, R. K. (2000). <a href="https://cstl-csm.semo.edu/xzhang/Class%20Folder/CS280/Workbook_HTML/FLOATING_tut.htm">Tutorial: Floating-Point Binary</a> [Accessed: 2021-05-08] <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:11" role="doc-endnote"><p>Finley, T. (2000). <a href="https://www.cs.cornell.edu/~tomf/notes/cps104/twoscomp.html">Two’s Complement</a> [Accessed: 2021-05-08] <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:12" role="doc-endnote"><p>Chadwick, R. (2021). <a href="https://ryanstutorials.net/binary-tutorial/binary-negative-numbers.php">Binary Negative Numbers!</a> [Accessed: 2021-05-08] <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:13" role="doc-endnote"><p>Koretskyi, M. (2016). <a href="https://indepth.dev/posts/1018/the-mechanics-behind-exponent-bias-in-floating-point">The mechanics behind exponent bias in floating point</a> [Accessed: 2021-05-08] <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:14" role="doc-endnote"><p>Oracle. (2000). <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_math.html">IEEE Arithmetic</a> [Accessed: 2021-05-09] <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:15" role="doc-endnote"><p>IBM. (2019). <a href="https://www.ibm.com/docs/en/i/7.4?topic=numbers-subnormal-underflow">Subnormal numbers and underflow</a> [Accessed: 2021-05-09] <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:16" role="doc-endnote"><p>Stan Development Project. (2019). <a href="https://mc-stan.org/docs/2_26/stan-users-guide/floating-point-representations.html">Floating-point representations</a> [Accessed: 2021-05-09] <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:17" role="doc-endnote"><p>Harries, I. (2003). <a href="https://www.doc.ic.ac.uk/~eedwards/compsys/float/nan.html">Infinity and NaNs</a> [Accessed: 2021-05-09] <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:18" role="doc-endnote"><p>Lawlor, O. (2011). <a href="https://www.cs.uaf.edu/2011/fall/cs301/lecture/11_09_weird_floats.html">Weird Floating-Point Numbers: Infinity, Denormal, and NaN</a> [Accessed: 2021-05-09] <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/linux/'>Linux</a>, <a href='/categories/mathematics/'>Mathematics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/bash/" class="post-tag no-text-decoration" >bash</a> <a href="/tags/linux/" class="post-tag no-text-decoration" >linux</a> <a href="/tags/scripting/" class="post-tag no-text-decoration" >scripting</a> <a href="/tags/mathematics/" class="post-tag no-text-decoration" >mathematics</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Exploring IEEE 754 Arithmetic - Mairi's Blog&url=https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Exploring IEEE 754 Arithmetic - Mairi's Blog&u=https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Exploring IEEE 754 Arithmetic - Mairi's Blog&url=https://tmairi.github.io/posts/exploring-ieee-754-arithmetic/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/dissecting-the-ad1-file-format/">Dissecting the AD1 File Format</a><li><a href="/posts/exploring-ieee-754-arithmetic/">Exploring IEEE 754 Arithmetic</a><li><a href="/posts/LNKPARSER/">Forensic BASH Scripting: LNK Parsing</a><li><a href="/posts/forensic-aquisition-with-dd-tools/">Forensic Acquisition with DD Tools</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/dfir/">dfir</a> <a class="post-tag" href="/tags/bash/">bash</a> <a class="post-tag" href="/tags/scripting/">scripting</a> <a class="post-tag" href="/tags/mathematics/">mathematics</a> <a class="post-tag" href="/tags/windows/">windows</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/LNKPARSER/"><div class="card-body"> <span class="timeago small" > Apr 8 <i class="unloaded">2021-04-08T20:56:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Forensic BASH Scripting: LNK Parsing</h3><div class="text-muted small"><p> Forensic BASH script: lnkparser Observation Recently, while probing a Windows image file in my spare time, I came across a plethora of user-generated Windows shortcut files, also known as LNK fil...</p></div></div></a></div><div class="card"> <a href="/posts/forensic-aquisition-with-dd-tools/"><div class="card-body"> <span class="timeago small" > Jan 28 <i class="unloaded">2018-01-28T00:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Forensic Acquisition with DD Tools</h3><div class="text-muted small"><p> Foreword This article will be focusing on the usage of the Linux tool dd in the forensic imaging process, along with several tools that have been derived from it. In addition to briefly covering t...</p></div></div></a></div><div class="card"> <a href="/posts/extracting-alternate-data-streams-with-linux/"><div class="card-body"> <span class="timeago small" > Jun 22 <i class="unloaded">2018-06-22T01:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Extracting Alternate Data Streams with Linux</h3><div class="text-muted small"><p> Foreword This article will be covering a feature of the NTFS file system known as the Alternate Data Stream (ADS), focusing on how to properly identify and extract these data streams from an NTFS ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/LNKPARSER/" class="btn btn-outline-primary" prompt="Older"><p>Forensic BASH Scripting: LNK Parsing</p></a> <a href="/posts/dissecting-the-ad1-file-format/" class="btn btn-outline-primary" prompt="Newer"><p>Dissecting the AD1 File Format</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/AstrumMairi">Mairi</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/dfir/">dfir</a> <a class="post-tag" href="/tags/bash/">bash</a> <a class="post-tag" href="/tags/scripting/">scripting</a> <a class="post-tag" href="/tags/mathematics/">mathematics</a> <a class="post-tag" href="/tags/windows/">windows</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://tmairi.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
